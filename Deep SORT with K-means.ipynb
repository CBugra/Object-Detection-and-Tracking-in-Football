{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x512 20 persons, 46.7ms\n",
      "Speed: 2.5ms preprocess, 46.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.9ms\n",
      "Speed: 2.6ms preprocess, 39.9ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.9ms\n",
      "Speed: 2.0ms preprocess, 39.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.1ms\n",
      "Speed: 2.0ms preprocess, 40.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.8ms\n",
      "Speed: 2.0ms preprocess, 39.8ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.5ms\n",
      "Speed: 2.1ms preprocess, 39.5ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.8ms\n",
      "Speed: 1.1ms preprocess, 40.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.9ms\n",
      "Speed: 2.0ms preprocess, 39.9ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 41.3ms\n",
      "Speed: 1.0ms preprocess, 41.3ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.7ms\n",
      "Speed: 2.0ms preprocess, 40.7ms inference, 2.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 42.6ms\n",
      "Speed: 1.0ms preprocess, 42.6ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.0ms\n",
      "Speed: 2.0ms preprocess, 40.0ms inference, 4.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 42.0ms\n",
      "Speed: 1.0ms preprocess, 42.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 40.4ms\n",
      "Speed: 1.0ms preprocess, 40.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 18 persons, 40.0ms\n",
      "Speed: 2.1ms preprocess, 40.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 39.9ms\n",
      "Speed: 2.0ms preprocess, 39.9ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 41.0ms\n",
      "Speed: 2.0ms preprocess, 41.0ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 40.7ms\n",
      "Speed: 1.0ms preprocess, 40.7ms inference, 2.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 19 persons, 42.5ms\n",
      "Speed: 2.0ms preprocess, 42.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 42.4ms\n",
      "Speed: 1.0ms preprocess, 42.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 40.5ms\n",
      "Speed: 2.0ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.8ms\n",
      "Speed: 1.0ms preprocess, 40.8ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 42.0ms\n",
      "Speed: 2.0ms preprocess, 42.0ms inference, 3.5ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 20 persons, 42.3ms\n",
      "Speed: 2.0ms preprocess, 42.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.0ms\n",
      "Speed: 2.0ms preprocess, 40.0ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 1 sports ball, 42.1ms\n",
      "Speed: 1.0ms preprocess, 42.1ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.8ms\n",
      "Speed: 1.0ms preprocess, 42.8ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.8ms\n",
      "Speed: 1.0ms preprocess, 42.8ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 43.0ms\n",
      "Speed: 1.0ms preprocess, 43.0ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 42.3ms\n",
      "Speed: 1.0ms preprocess, 42.3ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 41.5ms\n",
      "Speed: 1.4ms preprocess, 41.5ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 41.5ms\n",
      "Speed: 1.0ms preprocess, 41.5ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 40.4ms\n",
      "Speed: 2.0ms preprocess, 40.4ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.9ms\n",
      "Speed: 2.0ms preprocess, 42.9ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.9ms\n",
      "Speed: 2.0ms preprocess, 42.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.7ms\n",
      "Speed: 2.0ms preprocess, 42.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 41.2ms\n",
      "Speed: 1.0ms preprocess, 41.2ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 43.5ms\n",
      "Speed: 0.0ms preprocess, 43.5ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 40.7ms\n",
      "Speed: 1.0ms preprocess, 40.7ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 41.4ms\n",
      "Speed: 1.0ms preprocess, 41.4ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 41.0ms\n",
      "Speed: 1.0ms preprocess, 41.0ms inference, 1.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.0ms\n",
      "Speed: 2.0ms preprocess, 40.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 40.1ms\n",
      "Speed: 1.0ms preprocess, 40.1ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 40.0ms\n",
      "Speed: 2.0ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 40.7ms\n",
      "Speed: 2.0ms preprocess, 40.7ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.0ms\n",
      "Speed: 2.0ms preprocess, 40.0ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 43.6ms\n",
      "Speed: 1.0ms preprocess, 43.6ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 43.8ms\n",
      "Speed: 1.0ms preprocess, 43.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 43.4ms\n",
      "Speed: 2.0ms preprocess, 43.4ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 43.3ms\n",
      "Speed: 1.0ms preprocess, 43.3ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 21 persons, 40.2ms\n",
      "Speed: 1.0ms preprocess, 40.2ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 39.0ms\n",
      "Speed: 1.0ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 39.7ms\n",
      "Speed: 2.0ms preprocess, 39.7ms inference, 0.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.5ms\n",
      "Speed: 1.0ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.0ms\n",
      "Speed: 2.0ms preprocess, 42.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 40.0ms\n",
      "Speed: 1.0ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 42.5ms\n",
      "Speed: 2.0ms preprocess, 42.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 40.9ms\n",
      "Speed: 1.0ms preprocess, 40.9ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 24 persons, 41.0ms\n",
      "Speed: 2.5ms preprocess, 41.0ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.7ms\n",
      "Speed: 1.0ms preprocess, 40.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 40.5ms\n",
      "Speed: 2.2ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 43.2ms\n",
      "Speed: 2.0ms preprocess, 43.2ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.3ms\n",
      "Speed: 2.0ms preprocess, 42.3ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 41.7ms\n",
      "Speed: 1.0ms preprocess, 41.7ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 43.5ms\n",
      "Speed: 1.0ms preprocess, 43.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 22 persons, 42.5ms\n",
      "Speed: 2.0ms preprocess, 42.5ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 288x512 23 persons, 42.5ms\n",
      "Speed: 1.0ms preprocess, 42.5ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 512)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "# Load the YOLOv8 model to run on CPU\n",
    "model = YOLO(r'C:\\Users\\hp\\Desktop\\Master_Thesis_Project\\Thesis Project\\Final Thesis Project\\New_Run\\New_Run\\Scenario 1\\train\\train\\weights\\best.pt')\n",
    "\n",
    "# Initialize the Deep SORT tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=150,       # If an object doesn't re-enter the frame within 150 frames, assign a new ID\n",
    "    n_init=5,          # Increase the number of confirmations required to reduce false positives\n",
    "    nn_budget=100,     # Store more past embeddings to improve ID accuracy\n",
    "    max_iou_distance=0.30  # Lower IoU threshold for more precise object matching\n",
    ")\n",
    "\n",
    "# Open your video source\n",
    "video_path = r'C:\\Users\\hp\\Desktop\\Master_Thesis_Project\\Dataset\\Filtered_Video Dataset\\filtered_video_4.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set parameters for saving the output video\n",
    "output_video_path = r'C:\\Users\\hp\\Desktop\\Master_Thesis_Project\\Thesis Project\\Final Thesis Project\\Video & Image Output\\k_means_output4.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = None\n",
    "\n",
    "# Function to increase brightness and saturation of a color\n",
    "def adjust_brightness(color, brightness_factor=1.0, saturation_factor=3.0):\n",
    "    \"\"\"Increase the brightness and saturation of a color.\"\"\"\n",
    "    # Increase the brightness of the color\n",
    "    brightened = tuple(min(int(c * brightness_factor), 255) for c in color)\n",
    "    \n",
    "    # Increase saturation by amplifying color differences\n",
    "    max_value = max(brightened)\n",
    "    saturated = tuple(min(int(c * saturation_factor + max_value * (1 - saturation_factor)), 255) for c in brightened)\n",
    "    \n",
    "    return saturated\n",
    "\n",
    "# Function to perform color clustering\n",
    "def get_dominant_color(image, k=1):\n",
    "    \"\"\"Find dominant colors using K-Means.\"\"\"\n",
    "    # Return the average color if the image is too small or empty\n",
    "    if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:\n",
    "        return tuple(map(int, image.mean(axis=(0, 1)))) if image.size != 0 else (0, 0, 0)  # Get the average color\n",
    "\n",
    "    # Reshape the image from (H, W, 3) to (H*W, 3)\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    if len(pixels) == 0:\n",
    "        return (0, 0, 0)\n",
    "\n",
    "    # Use K-Means clustering to find dominant colors\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Find the number of pixels in each cluster\n",
    "    counts = Counter(kmeans.labels_)\n",
    "\n",
    "    # Select the color of the largest cluster\n",
    "    dominant_color = kmeans.cluster_centers_[counts.most_common(1)[0][0]]\n",
    "\n",
    "    # Convert the color to integer format and return in (B, G, R) format\n",
    "    return tuple(map(int, dominant_color))  # Return in BGR format\n",
    "\n",
    "# Function to define a region of interest (ROI) from a bounding box\n",
    "def get_roi(frame, x1, y1, x2, y2):\n",
    "    \"\"\"Define a central ROI within a bounding box.\"\"\"\n",
    "    roi_width = int((x2 - x1) * 0.5)  # Select an ROI that is 50% of the bounding box width\n",
    "    roi_height = int((y2 - y1) * 0.5)  # Select an ROI that is 50% of the bounding box height\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2  # Find the center of the bounding box\n",
    "\n",
    "    # Calculate the boundaries of the ROI\n",
    "    roi_x1 = max(center_x - roi_width // 2, 0)\n",
    "    roi_y1 = max(center_y - roi_height // 2, 0)\n",
    "    roi_x2 = min(center_x + roi_width // 2, frame.shape[1])\n",
    "    roi_y2 = min(center_y + roi_height // 2, frame.shape[0])\n",
    "\n",
    "    # Crop the ROI from the frame\n",
    "    return frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "# Function to determine the average dominant color from multiple ROIs\n",
    "def get_average_dominant_color(frame, x1, y1, x2, y2):\n",
    "    \"\"\"Find the average dominant color from multiple ROIs within a bounding box.\"\"\"\n",
    "    # Define the ROIs (top, bottom, left, right)\n",
    "    rois = []\n",
    "    rois.append(get_roi(frame, x1, y1, x2, y1 + (y2 - y1) // 2))  # Top half\n",
    "    rois.append(get_roi(frame, x1, y1 + (y2 - y1) // 2, x2, y2))  # Bottom half\n",
    "    rois.append(get_roi(frame, x1, y1, x1 + (x2 - x1) // 2, y2))  # Left half\n",
    "    rois.append(get_roi(frame, x1 + (x2 - x1) // 2, y1, x2, y2))  # Right half\n",
    "\n",
    "    # Find the dominant color for each ROI\n",
    "    dominant_colors = [get_dominant_color(roi) for roi in rois]\n",
    "\n",
    "    # Calculate the average dominant color\n",
    "    avg_color = np.mean(dominant_colors, axis=0)\n",
    "\n",
    "    return tuple(map(int, avg_color))  # Return the average dominant color\n",
    "\n",
    "# Process the video frame by frame\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform object detection using YOLOv8 (running on CPU)\n",
    "    results = model(frame)\n",
    "\n",
    "    # Format detected objects\n",
    "    detections = []\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            # Get the coordinates (x1, y1, x2, y2) and detection confidence from YOLOv8\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "            w = x2 - x1  # Width\n",
    "            h = y2 - y1  # Height\n",
    "            confidence = box.conf[0].item()\n",
    "            class_id = int(box.cls[0])\n",
    "\n",
    "            # If class_id is for a person (0), track the person\n",
    "            if class_id == 0:\n",
    "                # Format for DeepSORT: ([x1, y1, w, h], confidence, class_id)\n",
    "                detections.append(([x1, y1, w, h], confidence, class_id))\n",
    "\n",
    "    # Track the objects using Deep SORT\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # Draw tracked objects and add color information\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()  # Return [left, top, right, bottom]\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "\n",
    "        # Ensure that the boundary values are valid\n",
    "        if x1 < 0: x1 = 0\n",
    "        if y1 < 0: y1 = 0\n",
    "        if x2 > frame.shape[1]: x2 = frame.shape[1]\n",
    "        if y2 > frame.shape[0]: y2 = frame.shape[0]\n",
    "\n",
    "        # Determine the average dominant color from multiple regions\n",
    "        avg_dominant_color = get_average_dominant_color(frame, x1, y1, x2, y2)\n",
    "\n",
    "        # Make the dominant color more vivid by increasing brightness and saturation\n",
    "        avg_dominant_color = adjust_brightness(avg_dominant_color)\n",
    "\n",
    "        # Use BGR format for OpenCV (draw the bounding box and add color info)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), avg_dominant_color, 2)\n",
    "        # Draw the ID number in the same color as the bounding box\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, avg_dominant_color, 2)\n",
    "\n",
    "    # Write the output video\n",
    "    if out is None:\n",
    "        height, width, _ = frame.shape\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "# Release the video source and output\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
